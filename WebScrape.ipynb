{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2588ba5-264f-47fb-8032-6f2b6392648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/21\n",
      "Position: 1, Club: Manchester City, Points: 86\n",
      "Position: 2, Club: Manchester United, Points: 74\n",
      "Position: 3, Club: Liverpool, Points: 69\n",
      "Position: 4, Club: Chelsea, Points: 67\n",
      "Position: 5, Club: Leicester City, Points: 66\n",
      "Position: 6, Club: West Ham United, Points: 65\n",
      "Position: 7, Club: Tottenham Hotspur, Points: 62\n",
      "Position: 8, Club: Arsenal, Points: 61\n",
      "Position: 9, Club: Leeds United, Points: 59\n",
      "Position: 10, Club: Everton, Points: 59\n",
      "Position: 11, Club: Aston Villa, Points: 55\n",
      "Position: 12, Club: Newcastle United, Points: 45\n",
      "Position: 13, Club: Wolverhampton Wanderers, Points: 45\n",
      "Position: 14, Club: Crystal Palace, Points: 44\n",
      "Position: 15, Club: Southampton, Points: 43\n",
      "Position: 16, Club: Brighton and Hove Albion, Points: 41\n",
      "Position: 17, Club: Burnley, Points: 39\n",
      "Position: 18, Club: Fulham, Points: 28\n",
      "Position: 19, Club: West Bromwich Albion, Points: 26\n",
      "Position: 20, Club: Sheffield United, Points: 23\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "#open the webpage and remove cookies and ads    \n",
    "\n",
    "url = \"https://www.premierleague.com/tables\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies =  driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "accept_cookies.click()\n",
    "#remove = driver.find_element(By.XPATH, '//*[@id=\"advertClose\"]')\n",
    "#remove.click()\n",
    "\n",
    "#years = ['2020/21','2021/22']\n",
    "years = ['2020/21']\n",
    "i=2019\n",
    "\n",
    "for season in years:\n",
    "    i=i+1\n",
    "    print(season)\n",
    "\n",
    "    \n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\LeagueTable\" + str(i) + \".csv\", 'w')\n",
    "    file1.write('Position, Club Name, Points, Goals For\\n')\n",
    "   \n",
    "\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']  \")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(\n",
    "          EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\"))\n",
    "    )\n",
    "    year_to_select.click()\n",
    "    #Wait for page to refresh\n",
    "   \n",
    "    time.sleep(10)\n",
    "\n",
    "    #Get the data\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "   \n",
    "    # Find the body of the league table\n",
    "    table_body = soup.find('tbody', class_='league-table__tbody')\n",
    "\n",
    "    # Iterate through each row in the table body\n",
    "    for row in table_body.find_all('tr'):\n",
    "       if row.find('td', class_='league-table__pos'):\n",
    "        #position = row.find('td', class_='league-table__pos').text.strip()\n",
    "        position = row.find('span', class_='league-table__value value').text.strip()    \n",
    "        club_name = row.find('span', class_='league-table__team-name--long').text.strip()\n",
    "        #played = row.find_all('td')[2].text.strip()\n",
    "        #won = row.find_all('td')[3].text.strip()\n",
    "        #drawn = row.find_all('td')[4].text.strip()\n",
    "        #lost = row.find_all('td')[5].text.strip()\n",
    "        gf = row.find_all('td')[6].text.strip()\n",
    "        #ga = row.find_all('td')[7].text.strip()\n",
    "        #gd = row.find_all('td')[8].text.strip()\n",
    "        points = row.find('td', class_='league-table__points').text.strip() if row.find('td', class_='league-table__points') else 'N/A'\n",
    "        #print(f'{position} {club_name} - Played: {played}, Wins: {won}, Draws: {drawn}, Losses: {lost}, GF: {gf}, GA: {ga}, GD: {gd}, Points: {points}')\n",
    "        print(f'Position: {position}, Club: {club_name}, Points: {points}')\n",
    "        file1.write(f'{position}, {club_name}, {points}, {gf}\\n')\n",
    "\n",
    "    file1.close()\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62b4c5-9781-4491-8142-4e02fd167e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87f2010e-56aa-4806-99c5-e02eabc23a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/21\n",
      "Name: Tammy Abraham, Position: Forward, Country: England\n",
      "Club: Chelsea, Height: 190cm, DoB: 02/10/1997\n",
      "Name: Ché Adams, Position: Forward, Country: Scotland\n",
      "Club: Southampton, Height: 175cm, DoB: 13/07/1996\n",
      "Name: Tosin Adarabioyo, Position: Defender, Country: England\n",
      "Club: Fulham, Height: 196cm, DoB: 24/09/1997  (26)\n",
      "Name: Dennis Adeniran, Position: Midfielder, Country: England\n",
      "Club: Everton, Height: 180cm, DoB: 02/01/1999\n",
      "Name: Adrián, Position: Goalkeeper, Country: Spain\n",
      "Club: Liverpool, Height: 190cm, DoB: 03/01/1987  (37)\n",
      "Name: Adrien Silva, Position: Midfielder, Country: Portugal\n",
      "Club: Leicester City, Height: 171cm, DoB: 15/03/1989\n",
      "Name: Oladapo Afolayan, Position: Forward, Country: England\n",
      "Club: West Ham United, Height: 180cm, DoB: 11/09/1997\n",
      "Name: Sergio Agüero, Position: Forward, Country: Argentina\n",
      "Club: Manchester City, Height: 173cm, DoB: 02/06/1988\n",
      "Name: Ahmed El Mohamady, Position: Defender, Country: Egypt\n",
      "Club: Aston Villa, Height: 183cm, DoB: 09/09/1987\n",
      "Name: Ahmed Hegazy, Position: Defender, Country: Egypt\n",
      "Club: West Bromwich Albion, Height: 193cm, DoB: 25/01/1991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_premier_league_players(soup,player_file):\n",
    "   \n",
    "    #Extract and print information for each player\n",
    "\n",
    "    player_rows = soup.find_all('tr', class_='player')\n",
    "\n",
    "    for row in player_rows:\n",
    "        player_name_link = row.find('a', class_='player__name')\n",
    "        if player_name_link:\n",
    "            player_name = player_name_link.get_text(strip=True)\n",
    "            player_href = player_name_link.get('href')\n",
    "       \n",
    "        position = row.find('td', class_='player__position').get_text(strip=True)\n",
    "        country = row.find('span', class_='player__country').get_text(strip=True)\n",
    "        \n",
    "        \n",
    "        print(f'Name: {player_name}, Position: {position}, Country: {country}')\n",
    "        player_file.write(f\"Name: {player_name}, Position: {position}, Country: {country},\")\n",
    "        \n",
    "        \n",
    "        scrape_premier_league_players_stats_2(player_href,player_file)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def scrape_premier_league_players_stats_2(player_page,player_file):\n",
    "\n",
    "    player_page=\"https:\"+player_page\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(player_page)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    " \n",
    "          \n",
    "    labels = soup.find_all('div', class_='player-overview__label')\n",
    "    clubs_rows = soup.find_all('tr', class_='table player-club-history__table-row')\n",
    "                            \n",
    "\n",
    "    player_stats = {}\n",
    "    club_name=\"\"\n",
    "    height=\"\"\n",
    "    DoB=\"\"\n",
    "\n",
    "\n",
    "    for row in clubs_rows:\n",
    "        season = row.find('td', class_='player-club-history__season')\n",
    "        if season and season.get_text(strip=True) == '2020/2021':\n",
    "            club_name = row.find('span', class_='player-club-history__team-name').get_text(strip=True)\n",
    "            break\n",
    "           \n",
    "    #print(\"Club Name:\", club_name)\n",
    "\n",
    "\n",
    "    # Loop through all found labels to find \"Height\"\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"height\":\n",
    "            # Find the next relate4 div that has class 'player-overview__info'\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                height = info_div.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "    # Print the player height name\n",
    "    #print(\"Height:\", height)\n",
    "\n",
    "\n",
    "    # Loop through all found labels to find DoB\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"date of birth\":\n",
    "            # Find the next relate4 div that has class 'player-overview__info'\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                DoB = info_div.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "    # Print the player height name\n",
    "    #print(\"DoB:\", DoB)\n",
    "\n",
    "\n",
    "    #player_file.write(f\",Club: {club_name}, Height: {height}, DoB: {DoB}\\n\")\n",
    "    #player_file.write(f\",Club: {club_name}, Height: {height}, DoB: {DoB}\")\n",
    "\n",
    "    #print(f'Name: {player_name}, Position: {position}, Country: {country}, Club: {club_name}, Height: {height}, DoB: {DoB}')\n",
    "    #player_file.write(f\"Name: {player_name}, Position: {position}, Country: {country}, Club: {club_name}, Height: {height}, DoB: {DoB}\")\n",
    "\n",
    "    print(f'Club: {club_name}, Height: {height}, DoB: {DoB}')\n",
    "    player_file.write(f\"Club: {club_name}, Height: {height}, DoB: {DoB}\\n\")\n",
    "\n",
    "   \n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#open the webpage and remove cookies and ads    \n",
    "\n",
    "url = \"https://www.premierleague.com/players\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies =  driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "accept_cookies.click()\n",
    "#remove = driver.find_element(By.XPATH, '//*[@id=\"advertClose\"]')\n",
    "#remove.click()    \n",
    "\n",
    "#years = ['1992/93','1993/94']\n",
    "#i=1991\n",
    "\n",
    "#years = ['2020/21','2021/22']\n",
    "years = ['2020/21']\n",
    "i=2019\n",
    "\n",
    "for season in years:\n",
    "    i=i+1\n",
    "    print(season)\n",
    "\n",
    " #Open a file\n",
    "\n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\PlayerData\" + str(i) + \".csv\", 'w')\n",
    "\n",
    "   \n",
    "\n",
    "#Select the year to process\n",
    "\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']  \")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(\n",
    "          EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\"))\n",
    "    )\n",
    "    year_to_select.click()\n",
    "\n",
    "\n",
    "\n",
    "#Scroll to bottom and wait 60 seconds\n",
    "#driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "#time.sleep(60)\n",
    "   \n",
    "#Wait for page to refresh\n",
    "   \n",
    "    time.sleep(10)\n",
    "\n",
    "\n",
    "\n",
    "#Get the data\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scrape_premier_league_players(soup,file1)\n",
    "    print()\n",
    "\n",
    "#Close the file\n",
    "    file1.close()\n",
    "\n",
    "#  Close the browser when done\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398dc576-7e57-4a84-ab83-bcaf3e842598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8190b5c8-17bb-4eed-a677-1927bd39f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/21\n",
      "Name: Tammy Abraham, Position: Forward, Country: England\n",
      "Club: Chelsea, Height: 190cm, DoB: 02/10/1997, Goals: 6\n",
      "Name: Ché Adams, Position: Forward, Country: Scotland\n",
      "Club: Southampton, Height: 175cm, DoB: 13/07/1996, Goals: 9\n",
      "Name: Tosin Adarabioyo, Position: Defender, Country: England\n",
      "Club: Fulham, Height: 196cm, DoB: 24/09/1997  (26), Goals: 0\n",
      "Name: Dennis Adeniran, Position: Midfielder, Country: England\n",
      "Club: Everton, Height: 180cm, DoB: 02/01/1999, Goals: 0\n",
      "Name: Adrián, Position: Goalkeeper, Country: Spain\n",
      "Club: Liverpool, Height: 190cm, DoB: 03/01/1987  (37), Goals: 0\n",
      "Name: Adrien Silva, Position: Midfielder, Country: Portugal\n",
      "Club: Leicester City, Height: 171cm, DoB: 15/03/1989, Goals: 0\n",
      "Name: Oladapo Afolayan, Position: Forward, Country: England\n",
      "Club: West Ham United, Height: 180cm, DoB: 11/09/1997, Goals: 0\n",
      "Name: Sergio Agüero, Position: Forward, Country: Argentina\n",
      "Club: Manchester City, Height: 173cm, DoB: 02/06/1988, Goals: 4\n",
      "Name: Ahmed El Mohamady, Position: Defender, Country: Egypt\n",
      "Club: Aston Villa, Height: 183cm, DoB: 09/09/1987, Goals: 0\n",
      "Name: Ahmed Hegazy, Position: Defender, Country: Egypt\n",
      "Club: West Bromwich Albion, Height: 193cm, DoB: 25/01/1991, Goals: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_premier_league_players(soup,player_file):\n",
    "   \n",
    "    #Extract and print information for each player\n",
    "\n",
    "    player_rows = soup.find_all('tr', class_='player')\n",
    "\n",
    "    for row in player_rows:\n",
    "        player_name_link = row.find('a', class_='player__name')\n",
    "        if player_name_link:\n",
    "            player_name = player_name_link.get_text(strip=True)\n",
    "            player_href = player_name_link.get('href')\n",
    "       \n",
    "        position = row.find('td', class_='player__position').get_text(strip=True)\n",
    "        country = row.find('span', class_='player__country').get_text(strip=True)\n",
    "        \n",
    "        \n",
    "        print(f'Name: {player_name}, Position: {position}, Country: {country}')\n",
    "        player_file.write(f\"{player_name}, {position}, {country},\")\n",
    "        \n",
    "        \n",
    "        scrape_premier_league_players_stats_2(player_href,player_file)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def scrape_premier_league_players_stats_2(player_page,player_file):\n",
    "\n",
    "    player_page=\"https:\"+player_page\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(player_page)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    " \n",
    "          \n",
    "    labels = soup.find_all('div', class_='player-overview__label')\n",
    "    clubs_rows = soup.find_all('tr', class_='table player-club-history__table-row')\n",
    "                            \n",
    "\n",
    "    club_name=\"\"\n",
    "    height=\"\"\n",
    "    DoB=\"\"\n",
    "    goals=\"\"\n",
    "\n",
    "\n",
    "    for row in clubs_rows:\n",
    "        season = row.find('td', class_='player-club-history__season')\n",
    "        if season and season.get_text(strip=True) == '2020/2021':\n",
    "            club_name = row.find('span', class_='player-club-history__team-name').get_text(strip=True)\n",
    "            goals = row.find('td', class_='player-club-history__goals').get_text(strip=True)\n",
    "            break\n",
    "           \n",
    "\n",
    "\n",
    "    # Loop through all found labels to find \"Height\"\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"height\":\n",
    "            # Find the next relate4 div that has class 'player-overview__info'\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                height = info_div.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "\n",
    "    \n",
    "    # Loop through all found labels to find DoB\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"date of birth\":\n",
    "            # Find the next relate4 div that has class 'player-overview__info'\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                DoB = info_div.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "\n",
    "    print(f'Club: {club_name}, Height: {height}, DoB: {DoB}, Goals: {goals}')\n",
    "    player_file.write(f\"{club_name}, {height}, {DoB}, {goals}\\n\")\n",
    "\n",
    "   \n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#open the webpage and remove cookies and ads    \n",
    "\n",
    "url = \"https://www.premierleague.com/players\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies =  driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "accept_cookies.click()\n",
    "#remove = driver.find_element(By.XPATH, '//*[@id=\"advertClose\"]')\n",
    "#remove.click()    \n",
    "\n",
    "#years = ['1992/93','1993/94']\n",
    "#i=1991\n",
    "\n",
    "#years = ['2020/21','2021/22']\n",
    "years = ['2020/21']\n",
    "i=2019\n",
    "\n",
    "for season in years:\n",
    "    i=i+1\n",
    "    print(season)\n",
    "\n",
    " #Open a file\n",
    "\n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\PlayerData\" + str(i) + \".csv\", 'w')\n",
    "    file1.write('Player Name, Position, Nationality, Club, Height, DoB, Goals\\n')\n",
    "\n",
    "   \n",
    "\n",
    "#Select the year to process\n",
    "\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']  \")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(\n",
    "          EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\"))\n",
    "    )\n",
    "    year_to_select.click()\n",
    "\n",
    "\n",
    "\n",
    "#Scroll to bottom and wait 60 seconds\n",
    "#driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "#time.sleep(60)\n",
    "   \n",
    "#Wait for page to refresh\n",
    "   \n",
    "    time.sleep(10)\n",
    "\n",
    "\n",
    "\n",
    "#Get the data\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scrape_premier_league_players(soup,file1)\n",
    "    print()\n",
    "\n",
    "#Close the file\n",
    "    file1.close()\n",
    "\n",
    "#  Close the browser when done\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c21ff2-74de-44c5-a6f0-0e9cf9c463db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "#open the webpage and remove cookies and ads    \n",
    "\n",
    "url = \"https://www.premierleague.com/tables\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies =  driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "accept_cookies.click()\n",
    "#remove = driver.find_element(By.XPATH, '//*[@id=\"advertClose\"]')\n",
    "#remove.click()\n",
    "\n",
    "#years = ['2020/21','2021/22']\n",
    "years = ['2020/21']\n",
    "i=2019\n",
    "\n",
    "for season in years:\n",
    "    i=i+1\n",
    "    print(season)\n",
    "\n",
    "    \n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\LeagueTable\" + str(i) + \".csv\", 'w')\n",
    "    file1.write('Position, Club Name, Points, Goals For\\n')\n",
    "   \n",
    "\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']  \")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(\n",
    "          EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\"))\n",
    "    )\n",
    "    year_to_select.click()\n",
    "    #Wait for page to refresh\n",
    "   \n",
    "    time.sleep(10)\n",
    "\n",
    "    #Get the data\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "   \n",
    "    # Find the body of the league table\n",
    "    table_body = soup.find('tbody', class_='league-table__tbody')\n",
    "\n",
    "    # Iterate through each row in the table body\n",
    "    for row in table_body.find_all('tr'):\n",
    "       if row.find('td', class_='league-table__pos'):\n",
    "        #position = row.find('td', class_='league-table__pos').text.strip()\n",
    "        position = row.find('span', class_='league-table__value value').text.strip()    \n",
    "        club_name = row.find('span', class_='league-table__team-name--long').text.strip()\n",
    "        #played = row.find_all('td')[2].text.strip()\n",
    "        #won = row.find_all('td')[3].text.strip()\n",
    "        #drawn = row.find_all('td')[4].text.strip()\n",
    "        #lost = row.find_all('td')[5].text.strip()\n",
    "        gf = row.find_all('td')[6].text.strip()\n",
    "        #ga = row.find_all('td')[7].text.strip()\n",
    "        #gd = row.find_all('td')[8].text.strip()\n",
    "        points = row.find('td', class_='league-table__points').text.strip() if row.find('td', class_='league-table__points') else 'N/A'\n",
    "        #print(f'{position} {club_name} - Played: {played}, Wins: {won}, Draws: {drawn}, Losses: {lost}, GF: {gf}, GA: {ga}, GD: {gd}, Points: {points}')\n",
    "        print(f'Position: {position}, Club: {club_name}, Points: {points}')\n",
    "        file1.write(f'{position}, {club_name}, {points}, {gf}\\n')\n",
    "\n",
    "    file1.close()\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d4cb40-ff34-4288-af7e-cb40bdd3aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "#open the webpage and accept cookies \n",
    "\n",
    "url = \"https://www.premierleague.com/tables\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies =  driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "accept_cookies.click()\n",
    "remove = driver.find_element(By.XPATH, '//*[@id=\"advertClose\"]')\n",
    "remove.click()  \n",
    "\n",
    "\n",
    "#define the start and end years for full years of data and create empty list\n",
    "\n",
    "start_year = 1992\n",
    "end_year = 2022\n",
    "years = []\n",
    "\n",
    "#loop through the range of years and generate list of all years in correct format for filter\n",
    "for year in range(start_year, end_year + 1):\n",
    "    formatted_year = f\"{year}/{(year + 1) % 100:02d}\"\n",
    "    years.append(formatted_year)\n",
    "\n",
    "#variable for file naming\n",
    "i=1991\n",
    "\n",
    "#loop through all the years\n",
    "for season in years:\n",
    "    i=i+1\n",
    "\n",
    "    #create csv file with consistent naming and clear directory, to write scraped data into and create 'headers' \n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\LeagueData\\LeagueTable\" + str(i) + \".csv\", 'w')\n",
    "    file1.write('Position,Club Name,Points,Goals For\\n')\n",
    "   \n",
    "\n",
    "    #apply filter to access desired data\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']  \")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(\n",
    "          EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\"))\n",
    "    )\n",
    "    year_to_select.click()\n",
    "    \n",
    "    \n",
    "    #wait for page to refresh\n",
    "    time.sleep(10)\n",
    "\n",
    "    #get the data\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "   \n",
    "    #find the body of the league table\n",
    "    table_body = soup.find('tbody', class_='league-table__tbody')\n",
    "\n",
    "    #iterate through each row in the table body\n",
    "    for row in table_body.find_all('tr'):\n",
    "       if row.find('td', class_='league-table__pos'):\n",
    "        #position = row.find('td', class_='league-table__pos').text.strip()\n",
    "        position = row.find('span', class_='league-table__value value').text.strip()    \n",
    "        club_name = row.find('span', class_='league-table__team-name--long').text.strip()\n",
    "        gf = row.find_all('td')[6].text.strip()\n",
    "        points = row.find('td', class_='league-table__points').text.strip() if row.find('td', class_='league-table__points') else 'N/A'\n",
    "        \n",
    "        #write generated data into csv\n",
    "        file1.write(f'{position},{club_name},{points},{gf}\\n')\n",
    "\n",
    "    #close file and repeat loop\n",
    "    file1.close()\n",
    "\n",
    "#close webpage\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb5767-20df-49b7-bd0c-d97fe0d4d01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be88c57-459f-4afe-a823-c3ae2f44ba22",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: cannot determine loading status\nfrom disconnected: unable to send message to renderer\n  (Session info: chrome=124.0.6367.91)\nStacktrace:\n\tGetHandleVerifier [0x00007FF785151502+60802]\n\t(No symbol) [0x00007FF7850CAC02]\n\t(No symbol) [0x00007FF784F87CE4]\n\t(No symbol) [0x00007FF784F700D4]\n\t(No symbol) [0x00007FF784F7004C]\n\t(No symbol) [0x00007FF784F6E759]\n\t(No symbol) [0x00007FF784F6F3FF]\n\t(No symbol) [0x00007FF784F7E66E]\n\t(No symbol) [0x00007FF784F957CF]\n\t(No symbol) [0x00007FF784F9B38A]\n\t(No symbol) [0x00007FF784F6FB85]\n\t(No symbol) [0x00007FF784F953AE]\n\t(No symbol) [0x00007FF78501A582]\n\t(No symbol) [0x00007FF784FFA923]\n\t(No symbol) [0x00007FF784FC8FEC]\n\t(No symbol) [0x00007FF784FC9C21]\n\tGetHandleVerifier [0x00007FF78545411D+3217821]\n\tGetHandleVerifier [0x00007FF7854960B7+3488055]\n\tGetHandleVerifier [0x00007FF78548F03F+3459263]\n\tGetHandleVerifier [0x00007FF78520B846+823494]\n\t(No symbol) [0x00007FF7850D5F9F]\n\t(No symbol) [0x00007FF7850D0EC4]\n\t(No symbol) [0x00007FF7850D1052]\n\t(No symbol) [0x00007FF7850C18A4]\n\tBaseThreadInitThunk [0x00007FFD860C257D+29]\n\tRtlUserThreadStart [0x00007FFD87ECAA48+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 152\u001b[0m\n\u001b[0;32m    150\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[0;32m    151\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m scrape_premier_league_players(soup,file1)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#close the file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m, in \u001b[0;36mscrape_premier_league_players\u001b[1;34m(soup, player_file)\u001b[0m\n\u001b[0;32m     30\u001b[0m player_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#begin second function\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m scrape_premier_league_players_stats_2(player_href,player_file)\n",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m, in \u001b[0;36mscrape_premier_league_players_stats_2\u001b[1;34m(player_page, player_file)\u001b[0m\n\u001b[0;32m     42\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome()\n\u001b[0;32m     43\u001b[0m driver\u001b[38;5;241m.\u001b[39mmaximize_window()\n\u001b[1;32m---> 44\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(player_page)\n\u001b[0;32m     45\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[0;32m     46\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\NewAnaconda\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:356\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32m~\\NewAnaconda\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\NewAnaconda\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: unknown error: cannot determine loading status\nfrom disconnected: unable to send message to renderer\n  (Session info: chrome=124.0.6367.91)\nStacktrace:\n\tGetHandleVerifier [0x00007FF785151502+60802]\n\t(No symbol) [0x00007FF7850CAC02]\n\t(No symbol) [0x00007FF784F87CE4]\n\t(No symbol) [0x00007FF784F700D4]\n\t(No symbol) [0x00007FF784F7004C]\n\t(No symbol) [0x00007FF784F6E759]\n\t(No symbol) [0x00007FF784F6F3FF]\n\t(No symbol) [0x00007FF784F7E66E]\n\t(No symbol) [0x00007FF784F957CF]\n\t(No symbol) [0x00007FF784F9B38A]\n\t(No symbol) [0x00007FF784F6FB85]\n\t(No symbol) [0x00007FF784F953AE]\n\t(No symbol) [0x00007FF78501A582]\n\t(No symbol) [0x00007FF784FFA923]\n\t(No symbol) [0x00007FF784FC8FEC]\n\t(No symbol) [0x00007FF784FC9C21]\n\tGetHandleVerifier [0x00007FF78545411D+3217821]\n\tGetHandleVerifier [0x00007FF7854960B7+3488055]\n\tGetHandleVerifier [0x00007FF78548F03F+3459263]\n\tGetHandleVerifier [0x00007FF78520B846+823494]\n\t(No symbol) [0x00007FF7850D5F9F]\n\t(No symbol) [0x00007FF7850D0EC4]\n\t(No symbol) [0x00007FF7850D1052]\n\t(No symbol) [0x00007FF7850C18A4]\n\tBaseThreadInitThunk [0x00007FFD860C257D+29]\n\tRtlUserThreadStart [0x00007FFD87ECAA48+40]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "#dfefine function\n",
    "def scrape_premier_league_players(soup,player_file):\n",
    "   \n",
    "    #extract and print information for each player\n",
    "    player_rows = soup.find_all('tr', class_='player')\n",
    "\n",
    "    #scrape player name and access href to gain access to further stats\n",
    "    for row in player_rows:\n",
    "        player_name_link = row.find('a', class_='player__name')\n",
    "        if player_name_link:\n",
    "            player_name = player_name_link.get_text(strip=True)\n",
    "            player_href = player_name_link.get('href')\n",
    "\n",
    "        #scrape position and nationality\n",
    "        position = row.find('td', class_='player__position').get_text(strip=True)\n",
    "        country = row.find('span', class_='player__country').get_text(strip=True)\n",
    "        \n",
    "\n",
    "        #write scraped data into file\n",
    "        player_file.write(f\"{player_name}, {position}, {country},\")\n",
    "        \n",
    "        #begin second function\n",
    "        scrape_premier_league_players_stats_2(player_href,player_file)\n",
    "\n",
    "\n",
    "        \n",
    "#second function to gain data from linked player page\n",
    "def scrape_premier_league_players_stats_2(player_page,player_file):\n",
    "\n",
    "    player_page=\"https:\"+player_page\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(player_page)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    " \n",
    "    #finds player overview labels      \n",
    "    labels = soup.find_all('div', class_='player-overview__label')\n",
    "\n",
    "    #finds rows of player club history\n",
    "    clubs_rows = soup.find_all('tr', class_='table player-club-history__table-row')\n",
    "                            \n",
    "    #creates empty strings for desired variables\n",
    "    club_name=\"\"\n",
    "    height=\"\"\n",
    "    DoB=\"\"\n",
    "    goals=\"\"\n",
    "\n",
    "       \n",
    "    #finds club and goals for the same season as filter rather than total goals and most recent club\n",
    "    for row in clubs_rows:\n",
    "        current_season = row.find('td', class_='player-club-history__season')\n",
    "        if current_season and current_season.get_text(strip=True) == season[:-2] + season[:2] + season[-2:]:\n",
    "            club_name = row.find('span', class_='player-club-history__team-name').get_text(strip=True)\n",
    "            goals = row.find('td', class_='player-club-history__goals').get_text(strip=True)\n",
    "            break\n",
    "           \n",
    "\n",
    "\n",
    "    #loop through all found labels to find height\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"height\":\n",
    "            #find the next related div that has class 'player-overview__info'\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                height = info_div.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "\n",
    "    \n",
    "    #loop through all found labels to find DoB\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"date of birth\":\n",
    "            # Find the next related div that has class 'player-overview__info'\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                DoB = info_div.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "\n",
    "    #writes data into csv and then starts a new row in CSV for next player\n",
    "    player_file.write(f\"{club_name}, {height}, {DoB}, {goals}\\n\")\n",
    "\n",
    "    #closes webpage\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#open the webpage and accept cookies \n",
    "\n",
    "url = \"https://www.premierleague.com/players\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies =  driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "accept_cookies.click()\n",
    "remove = driver.find_element(By.XPATH, '//*[@id=\"advertClose\"]')\n",
    "remove.click()\n",
    "\n",
    "\n",
    "#list of years to collect data for\n",
    "years = ['2022/23']\n",
    "i=2012\n",
    "\n",
    "\n",
    "for season in years:\n",
    "    i=i+10\n",
    "\n",
    " #Open a file\n",
    "\n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\PlayerData/PlayerData\" + str(i) + \".csv\", 'w')\n",
    "    file1.write('Player Name, Position, Nationality, Club, Height, DoB, Goals\\n')\n",
    "\n",
    "   \n",
    "\n",
    "#Select the year to process\n",
    "    driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']  \")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(\n",
    "          EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\"))\n",
    "    )\n",
    "    year_to_select.click()\n",
    "\n",
    "\n",
    "\n",
    "    #Scroll to bottom and wait 60 seconds\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(60)\n",
    "   \n",
    "    #wait for page to refresh\n",
    "    #time.sleep(10)\n",
    "\n",
    "\n",
    "\n",
    "    #get the data\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scrape_premier_league_players(soup,file1)\n",
    "    print()\n",
    "\n",
    "    #close the file\n",
    "    file1.close()\n",
    "\n",
    "#close web page\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c46e7-c00c-4357-ad56-0252828a6229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b552cc8f-c644-4670-abf1-20f010e9a7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_premier_league_players(soup, player_file, start_index=424):\n",
    "    player_rows = soup.find_all('tr', class_='player')[start_index:]\n",
    "    for row in player_rows:\n",
    "        player_name_link = row.find('a', class_='player__name')\n",
    "        if player_name_link:\n",
    "            player_name = player_name_link.get_text(strip=True)\n",
    "            player_href = player_name_link.get('href')\n",
    "        position = row.find('td', class_='player__position').get_text(strip=True)\n",
    "        country = row.find('span', class_='player__country').get_text(strip=True)\n",
    "        player_file.write(f\"{player_name}, {position}, {country},\")\n",
    "        scrape_premier_league_players_stats_2(player_href, player_file)\n",
    "\n",
    "def scrape_premier_league_players_stats_2(player_page, player_file):\n",
    "    player_page = \"https:\" + player_page\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(player_page)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    labels = soup.find_all('div', class_='player-overview__label')\n",
    "    clubs_rows = soup.find_all('tr', class_='table player-club-history__table-row')\n",
    "    club_name = \"\"\n",
    "    height = \"\"\n",
    "    DoB = \"\"\n",
    "    goals = \"\"\n",
    "    for row in clubs_rows:\n",
    "        current_season = row.find('td', class_='player-club-history__season')\n",
    "        if current_season and current_season.get_text(strip=True) == season[:-2] + season[:2] + season[-2:]:\n",
    "            club_name = row.find('span', class_='player-club-history__team-name').get_text(strip=True)\n",
    "            goals = row.find('td', class_='player-club-history__goals').get_text(strip=True)\n",
    "            break\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"height\":\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                height = info_div.get_text(strip=True)\n",
    "                break\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"date of birth\":\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                DoB = info_div.get_text(strip=True)\n",
    "                break\n",
    "    player_file.write(f\"{club_name}, {height}, {DoB}, {goals}\\n\")\n",
    "    driver.quit()\n",
    "\n",
    "url = \"https://www.premierleague.com/players\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')))\n",
    "accept_cookies.click()\n",
    "remove = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"advertClose\"]')))\n",
    "remove.click()\n",
    "\n",
    "years = ['2022/23']\n",
    "i = 2012\n",
    "\n",
    "for season in years:\n",
    "    i += 10\n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\PlayerData/PlayerData\" + str(i) + \".csv\", 'a')\n",
    "    driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "    element = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']\")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\")))\n",
    "    year_to_select.click()\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(60)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scrape_premier_league_players(soup, file1, 424)\n",
    "    print()\n",
    "    file1.close()\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3217f00-1e97-41db-b660-00e270c250b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
