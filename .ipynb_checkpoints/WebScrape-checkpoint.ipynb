{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be88c57-459f-4afe-a823-c3ae2f44ba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#scrapes code from Premier League website\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "#define function\n",
    "def scrape_premier_league_players(soup,player_file):\n",
    "   \n",
    "    #extract and print information for each player\n",
    "    player_rows = soup.find_all('tr', class_='player')\n",
    "\n",
    "    #scrape player name and access href to gain access to further stats\n",
    "    for row in player_rows:\n",
    "        player_name_link = row.find('a', class_='player__name')\n",
    "        if player_name_link:\n",
    "            player_name = player_name_link.get_text(strip=True)\n",
    "            player_href = player_name_link.get('href')\n",
    "\n",
    "        #scrape position and nationality\n",
    "        position = row.find('td', class_='player__position').get_text(strip=True)\n",
    "        country = row.find('span', class_='player__country').get_text(strip=True)\n",
    "        \n",
    "        #write scraped data into file\n",
    "        player_file.write(f\"{player_name}, {position}, {country},\")\n",
    "        \n",
    "        #begin second function\n",
    "        scrape_premier_league_players_stats_2(player_href,player_file)\n",
    "\n",
    "        \n",
    "#second function to gain data from linked player page\n",
    "def scrape_premier_league_players_stats_2(player_page,player_file):\n",
    "\n",
    "    player_page=\"https:\"+player_page\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(player_page)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    " \n",
    "    #finds player overview labels      \n",
    "    labels = soup.find_all('div', class_='player-overview__label')\n",
    "\n",
    "    #finds rows of player club history\n",
    "    clubs_rows = soup.find_all('tr', class_='table player-club-history__table-row')\n",
    "                            \n",
    "    #creates empty strings for desired variables\n",
    "    club_name=\"\"\n",
    "    height=\"\"\n",
    "    DoB=\"\"\n",
    "    goals=\"\"\n",
    "\n",
    "       \n",
    "    #finds club and goals for the same season as filter rather than total goals and most recent club\n",
    "    for row in clubs_rows:\n",
    "        current_season = row.find('td', class_='player-club-history__season')\n",
    "        if current_season and current_season.get_text(strip=True) == season[:-2] + season[:2] + season[-2:]:\n",
    "            club_name = row.find('span', class_='player-club-history__team-name').get_text(strip=True)\n",
    "            goals = row.find('td', class_='player-club-history__goals').get_text(strip=True)\n",
    "            break\n",
    "           \n",
    "\n",
    "\n",
    "    #loop through all found labels to find height\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"height\":\n",
    "            #find the next related div that has class 'player-overview__info'\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                height = info_div.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "\n",
    "    \n",
    "    #loop through all found labels to find DoB\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"date of birth\":\n",
    "            # Find the next related div that has class 'player-overview__info'\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                DoB = info_div.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "\n",
    "    #writes data into csv and then starts a new row in CSV for next player\n",
    "    player_file.write(f\"{club_name}, {height}, {DoB}, {goals}\\n\")\n",
    "\n",
    "    #closes webpage\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#open the webpage and accept cookies and delete ads\n",
    "url = \"https://www.premierleague.com/players\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies =  driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "accept_cookies.click()\n",
    "remove = driver.find_element(By.XPATH, '//*[@id=\"advertClose\"]')\n",
    "remove.click()\n",
    "\n",
    "\n",
    "#list of years to collect data for\n",
    "years = ['1992/93', '2002/03', '2012/2013', '2022/23']\n",
    "i=1982\n",
    "\n",
    "\n",
    "for season in years:\n",
    "    i=i+10\n",
    "\n",
    " #open a file, name it and write into it\n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\PlayerData/PlayerData\" + str(i) + \".csv\", 'w')\n",
    "    file1.write('Player Name, Position, Nationality, Club, Height, DoB, Goals\\n')\n",
    "\n",
    "   \n",
    "\n",
    "#select the year to process by changing filter\n",
    "    driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']  \")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(\n",
    "          EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\"))\n",
    "    )\n",
    "    year_to_select.click()\n",
    "\n",
    "\n",
    "\n",
    "    #scroll to bottom and wait 60 seconds to allow all results to load in\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(60)\n",
    "   \n",
    "\n",
    "    #get the data\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scrape_premier_league_players(soup,file1)\n",
    "    print()\n",
    "\n",
    "    #close the file\n",
    "    file1.close()\n",
    "\n",
    "#close web page\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b552cc8f-c644-4670-abf1-20f010e9a7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#scrapes code from the 425th row of the table onwards as the scraping was interrupted by a drop in wifi\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_premier_league_players(soup, player_file, start_index=424):\n",
    "    player_rows = soup.find_all('tr', class_='player')[start_index:]\n",
    "    for row in player_rows:\n",
    "        player_name_link = row.find('a', class_='player__name')\n",
    "        if player_name_link:\n",
    "            player_name = player_name_link.get_text(strip=True)\n",
    "            player_href = player_name_link.get('href')\n",
    "        position = row.find('td', class_='player__position').get_text(strip=True)\n",
    "        country = row.find('span', class_='player__country').get_text(strip=True)\n",
    "        player_file.write(f\"{player_name}, {position}, {country},\")\n",
    "        scrape_premier_league_players_stats_2(player_href, player_file)\n",
    "\n",
    "def scrape_premier_league_players_stats_2(player_page, player_file):\n",
    "    player_page = \"https:\" + player_page\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(player_page)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    labels = soup.find_all('div', class_='player-overview__label')\n",
    "    clubs_rows = soup.find_all('tr', class_='table player-club-history__table-row')\n",
    "    club_name = \"\"\n",
    "    height = \"\"\n",
    "    DoB = \"\"\n",
    "    goals = \"\"\n",
    "    for row in clubs_rows:\n",
    "        current_season = row.find('td', class_='player-club-history__season')\n",
    "        if current_season and current_season.get_text(strip=True) == season[:-2] + season[:2] + season[-2:]:\n",
    "            club_name = row.find('span', class_='player-club-history__team-name').get_text(strip=True)\n",
    "            goals = row.find('td', class_='player-club-history__goals').get_text(strip=True)\n",
    "            break\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"height\":\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                height = info_div.get_text(strip=True)\n",
    "                break\n",
    "    for label in labels:\n",
    "        if label.get_text(strip=True).lower() == \"date of birth\":\n",
    "            info_div = label.find_next_sibling('div', class_='player-overview__info')\n",
    "            if info_div:\n",
    "                DoB = info_div.get_text(strip=True)\n",
    "                break\n",
    "    player_file.write(f\"{club_name}, {height}, {DoB}, {goals}\\n\")\n",
    "    driver.quit()\n",
    "\n",
    "url = \"https://www.premierleague.com/players\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')))\n",
    "accept_cookies.click()\n",
    "remove = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"advertClose\"]')))\n",
    "remove.click()\n",
    "\n",
    "years = ['2022/23']\n",
    "i = 2012\n",
    "\n",
    "for season in years:\n",
    "    i += 10\n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\PlayerData/PlayerData\" + str(i) + \".csv\", 'a')\n",
    "    driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "    element = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']\")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\")))\n",
    "    year_to_select.click()\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(60)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scrape_premier_league_players(soup, file1, 424)\n",
    "    print()\n",
    "    file1.close()\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417995e-fee5-4ce5-890b-dc7cc6663254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapes the Premier League overall standings\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "#open the webpage and accept cookies \n",
    "\n",
    "url = \"https://www.premierleague.com/tables\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "accept_cookies =  driver.find_element(By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "accept_cookies.click()\n",
    "remove = driver.find_element(By.XPATH, '//*[@id=\"advertClose\"]')\n",
    "remove.click()  \n",
    "\n",
    "\n",
    "#define the start and end years for full years of data and create empty list\n",
    "\n",
    "start_year = 1992\n",
    "end_year = 2022\n",
    "years = []\n",
    "\n",
    "#loop through the range of years and generate list of all years in correct format for filter\n",
    "for year in range(start_year, end_year + 1):\n",
    "    formatted_year = f\"{year}/{(year + 1) % 100:02d}\"\n",
    "    years.append(formatted_year)\n",
    "\n",
    "#variable for file naming\n",
    "i=1991\n",
    "\n",
    "#loop through all the years\n",
    "for season in years:\n",
    "    i=i+1\n",
    "\n",
    "    #create csv file with consistent naming and clear directory, to write scraped data into and create 'headers' \n",
    "    file1 = open(r\"C:\\Users\\matts\\Documents\\Empirical Project\\EmpiricalProject-720005462\\LeagueData\\LeagueTable\" + str(i) + \".csv\", 'w')\n",
    "    file1.write('Position,Club Name,Points,Goals For\\n')\n",
    "   \n",
    "\n",
    "    #apply filter to access desired data\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//div[@data-dropdown-current='compSeasons']  \")))\n",
    "    element.click()\n",
    "    year_to_select = WebDriverWait(driver, 10).until(\n",
    "          EC.element_to_be_clickable((By.XPATH, f\"//li[text()='{season}']\"))\n",
    "    )\n",
    "    year_to_select.click()\n",
    "    \n",
    "    \n",
    "    #wait for page to refresh\n",
    "    time.sleep(10)\n",
    "\n",
    "    #get the data\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "   \n",
    "    #find the body of the league table\n",
    "    table_body = soup.find('tbody', class_='league-table__tbody')\n",
    "\n",
    "    #iterate through each row in the table body\n",
    "    for row in table_body.find_all('tr'):\n",
    "       if row.find('td', class_='league-table__pos'):\n",
    "        #position = row.find('td', class_='league-table__pos').text.strip()\n",
    "        position = row.find('span', class_='league-table__value value').text.strip()    \n",
    "        club_name = row.find('span', class_='league-table__team-name--long').text.strip()\n",
    "        gf = row.find_all('td')[6].text.strip()\n",
    "        points = row.find('td', class_='league-table__points').text.strip() if row.find('td', class_='league-table__points') else 'N/A'\n",
    "        \n",
    "        #write generated data into csv\n",
    "        file1.write(f'{position},{club_name},{points},{gf}\\n')\n",
    "\n",
    "    #close file and repeat loop\n",
    "    file1.close()\n",
    "\n",
    "#close webpage\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
